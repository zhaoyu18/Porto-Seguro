{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(y, pred):\n",
    "    fpr, tpr, thr = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    g = 2 * metrics.auc(fpr, tpr) -1\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 59)\n",
      "(892816, 59)\n",
      "ps_ind_01 8 8\n",
      "ps_ind_02_cat 5 5\n",
      "ps_ind_03 12 12\n",
      "ps_ind_04_cat 3 3\n",
      "ps_ind_05_cat 8 8\n",
      "ps_ind_06_bin 2 2\n",
      "ps_ind_07_bin 2 2\n",
      "ps_ind_08_bin 2 2\n",
      "ps_ind_09_bin 2 2\n",
      "ps_ind_10_bin 2 2\n",
      "ps_ind_11_bin 2 2\n",
      "ps_ind_12_bin 2 2\n",
      "ps_ind_13_bin 2 2\n",
      "ps_ind_14 5 5\n",
      "ps_ind_15 14 14\n",
      "ps_ind_16_bin 2 2\n",
      "ps_ind_17_bin 2 2\n",
      "ps_ind_18_bin 2 2\n",
      "ps_reg_01 10 10\n",
      "ps_reg_02 19 19\n",
      "ps_reg_03 5013 5046\n",
      "ps_car_01_cat 13 13\n",
      "ps_car_02_cat 3 3\n",
      "ps_car_03_cat 3 3\n",
      "ps_car_04_cat 10 10\n",
      "ps_car_05_cat 3 3\n",
      "ps_car_06_cat 18 18\n",
      "ps_car_07_cat 3 3\n",
      "ps_car_08_cat 2 2\n",
      "ps_car_09_cat 6 6\n",
      "ps_car_10_cat 3 3\n",
      "ps_car_11_cat 104 104\n",
      "ps_car_11 5 5\n",
      "ps_car_12 184 201\n",
      "ps_car_13 70482 83769\n",
      "ps_car_14 850 885\n",
      "ps_car_15 15 15\n",
      "(595213, 408) (892816, 408)\n"
     ]
    }
   ],
   "source": [
    "# train and test data path\n",
    "DATA_TRAIN_PATH = '../kaggle/Porto Seguro/input/train.csv'\n",
    "DATA_TEST_PATH = '../kaggle/Porto Seguro/input/test.csv'\n",
    "\n",
    "train = pd.read_csv(DATA_TRAIN_PATH)\n",
    "test = pd.read_csv(DATA_TEST_PATH)\n",
    "test.insert(1,'target',0)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "x = pd.concat([train,test])\n",
    "x = x.reset_index(drop=True)\n",
    "unwanted = x.columns[x.columns.str.startswith('ps_calc_')]\n",
    "x.drop(unwanted,inplace=True,axis=1)\n",
    "\n",
    "features = x.columns[2:]\n",
    "categories = []\n",
    "for c in features:\n",
    "    trainno = len(x.loc[:train.shape[0],c].unique())\n",
    "    testno = len(x.loc[train.shape[0]:,c].unique())\n",
    "    print(c,trainno,testno)\n",
    "\n",
    "x.loc[:,'ps_reg_03'] = pd.cut(x['ps_reg_03'], 50,labels=False)\n",
    "x.loc[:,'ps_car_12'] = pd.cut(x['ps_car_12'], 50,labels=False)\n",
    "x.loc[:,'ps_car_13'] = pd.cut(x['ps_car_13'], 50,labels=False)\n",
    "x.loc[:,'ps_car_14'] =  pd.cut(x['ps_car_14'], 50,labels=False)\n",
    "x.loc[:,'ps_car_15'] =  pd.cut(x['ps_car_15'], 50,labels=False)\n",
    "\n",
    "test = x.loc[train.shape[0]:].copy()\n",
    "train = x.loc[:train.shape[0]].copy()\n",
    "\n",
    "#Always good to shuffle for SGD type optimizers\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train.drop('id',inplace=True,axis=1)\n",
    "test.drop('id',inplace=True,axis=1)\n",
    "\n",
    "target = train.target\n",
    "train.drop('target', inplace=True, axis=1)\n",
    "test.drop('target', inplace=True, axis=1)\n",
    "n_train = train.shape[0]\n",
    "\n",
    "train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "train_test.ps_reg_01 = (train_test.ps_reg_01 * 10).astype('int')\n",
    "train_test.ps_reg_02 = (train_test.ps_reg_02 * 10).astype('int')\n",
    "\n",
    "dummy_datas = []\n",
    "dummy_df = pd.DataFrame()\n",
    "for col in train_test.columns:\n",
    "    dummy = pd.get_dummies(train_test[col].astype('category'))\n",
    "    columns = dummy.columns.astype(str).tolist()\n",
    "    columns = [col + '_' + w for w in columns]\n",
    "    dummy.columns = columns\n",
    "    dummy_df = pd.concat((dummy_df, dummy), axis=1)\n",
    "    dummy_datas.append(dummy.values)\n",
    "\n",
    "train = dummy_df.values[:n_train, :]\n",
    "test = dummy_df.values[n_train:, :]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "indexs = np.arange(train.shape[0])\n",
    "idx_train, idx_val = train_test_split(indexs, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((476170,), (119043,))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train.shape, idx_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import InputSpec, Layer, Input, Dense, merge, Conv1D, Embedding\n",
    "from keras.layers import Lambda, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(v_matrixs):\n",
    "    v_size = v_datas[0].shape[1]\n",
    "    inputs = []\n",
    "    concates = []\n",
    "    \n",
    "    for i, matrix in enumerate(v_matrixs):\n",
    "        seq_cat = Input(shape=(matrix.shape[0],))\n",
    "        embedding = Dense(v_size, name=\"emb_{}\".format(i), use_bias=False)\n",
    "        embedding.trainable = True\n",
    "        cat = embedding(seq_cat)\n",
    "        inputs.append(seq_cat)\n",
    "        concates.append(cat)\n",
    "    \n",
    "    merge = concatenate(concates)\n",
    "    \n",
    "    d = Dense(200, activation=\"relu\")(merge)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dropout(rate=0.35)(d)\n",
    "    \n",
    "    d = Dense(100, activation=\"relu\")(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dropout(rate=0.25)(d)\n",
    "\n",
    "    d = Dense(50, activation=\"relu\")(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dropout(rate=0.15)(d)\n",
    "    \n",
    "#     d = Dense(25, activation=\"relu\")(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "#     d = Dropout(rate=0.1)(d)\n",
    "    \n",
    "    d = concatenate(([d]+inputs))\n",
    "    \n",
    "    pred = Dense(1, activation=\"sigmoid\")(d)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    for i, weights in enumerate(v_matrixs):\n",
    "        layer = model.get_layer(name=\"emb_{}\".format(i))\n",
    "        layer.set_weights([weights])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_field_sizes = []\n",
    "for dummy in dummy_datas:\n",
    "    dummy_field_sizes.append(dummy.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dummy_field_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_df = pd.read_csv('./fm_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.010434</td>\n",
       "      <td>-0.020143</td>\n",
       "      <td>-0.251859</td>\n",
       "      <td>-0.078678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.018816</td>\n",
       "      <td>0.114803</td>\n",
       "      <td>0.057687</td>\n",
       "      <td>0.013324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.096245</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.062295</td>\n",
       "      <td>0.072438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.004310</td>\n",
       "      <td>-0.001851</td>\n",
       "      <td>0.063845</td>\n",
       "      <td>0.039387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.051077</td>\n",
       "      <td>-0.023013</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>-0.029234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0 -0.010434 -0.020143 -0.251859 -0.078678\n",
       "1 -0.018816  0.114803  0.057687  0.013324\n",
       "2  0.096245  0.002964  0.062295  0.072438\n",
       "3 -0.004310 -0.001851  0.063845  0.039387\n",
       "4 -0.051077 -0.023013  0.005236 -0.029234"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_datas = []\n",
    "offset = 0\n",
    "for size in dummy_field_sizes:\n",
    "    v_datas.append(v_df.values[offset:offset+size])\n",
    "    offset += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_datas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476170,)\n",
      "Train on 476170 samples, validate on 119043 samples\n",
      "Epoch 1/10\n",
      "score:  0.0493679805914 0.0734040055109\n",
      "13s - loss: 0.2111 - val_loss: 0.1573\n",
      "Epoch 2/10\n",
      "score:  0.243046621919 0.265277697923\n",
      "11s - loss: 0.1531 - val_loss: 0.1538\n",
      "Epoch 3/10\n",
      "score:  0.294775420047 0.294596315598\n",
      "11s - loss: 0.1522 - val_loss: 0.1527\n",
      "Epoch 4/10\n",
      "score:  0.303788230392 0.298553193137\n",
      "11s - loss: 0.1519 - val_loss: 0.1523\n",
      "Epoch 5/10\n",
      "score:  0.306753967279 0.297540188293\n",
      "11s - loss: 0.1517 - val_loss: 0.1523\n",
      "Epoch 6/10\n",
      "score:  0.314360081503 0.297568527991\n",
      "11s - loss: 0.1514 - val_loss: 0.1524\n",
      "Epoch 7/10\n",
      "score:  0.319014417594 0.293199657817\n",
      "11s - loss: 0.1512 - val_loss: 0.1526\n",
      "Epoch 8/10\n",
      "score:  0.327335992062 0.297624665914\n",
      "11s - loss: 0.1510 - val_loss: 0.1523\n",
      "Epoch 9/10\n",
      "score:  0.331027581943 0.295546690825\n",
      "11s - loss: 0.1509 - val_loss: 0.1525\n",
      "Epoch 10/10\n",
      "score:  0.338530869086 0.290425443083\n",
      "11s - loss: 0.1507 - val_loss: 0.1525\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "bst_evaluate = 0\n",
    "\n",
    "train_fold_datas = []\n",
    "for dummy in dummy_datas:\n",
    "    train_fold_cat_data = dummy[idx_train]\n",
    "    train_fold_datas.append(train_fold_cat_data)\n",
    "\n",
    "train_fold_target = target[idx_train]\n",
    "\n",
    "val_fold_datas = []\n",
    "for dummy in dummy_datas:\n",
    "    val_fold_cat_data = dummy[idx_val]\n",
    "    val_fold_datas.append(val_fold_cat_data)\n",
    "\n",
    "val_fold_target = target[idx_val]\n",
    "\n",
    "print(train_fold_target.shape)\n",
    "\n",
    "model = build_model(v_datas)\n",
    "\n",
    "def batch_evalue(batch, logs):\n",
    "    global model, bst_evaluate, scores\n",
    "    if logs['batch'] % 200 == 0 and logs['batch'] != 0:\n",
    "        val_pred = model.predict(val_fold_datas,\n",
    "                             batch_size=2048,\n",
    "                             verbose=2\n",
    "                            )\n",
    "        val_score = gini(val_fold_target, val_pred)\n",
    "\n",
    "        train_pred = model.predict(train_fold_datas,\n",
    "                             batch_size=2048,\n",
    "                             verbose=2\n",
    "                            )\n",
    "        train_score = gini(train_fold_target, train_pred)\n",
    "        print(\"score: \", train_score, val_score)\n",
    "        scores.append((train_score, val_score))\n",
    "\n",
    "batch_callback = LambdaCallback(on_batch_begin=batch_evalue)\n",
    "\n",
    "hist = model.fit(train_fold_datas,\n",
    "                 train_fold_target,\n",
    "                 validation_data=(val_fold_datas, val_fold_target),\n",
    "                 epochs = 10,\n",
    "                 batch_size = 2048,\n",
    "                 shuffle = True,\n",
    "                 verbose = 2,\n",
    "                 callbacks=[batch_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
