{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed()\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "import mxnet as mx\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and test data path\n",
    "DATA_TRAIN_PATH = '../input/train.csv'\n",
    "DATA_TEST_PATH = '../input/test.csv'\n",
    "\n",
    "def load_data(path_train=DATA_TRAIN_PATH, path_test=DATA_TEST_PATH):\n",
    "    train_loader = pd.read_csv(path_train, dtype={'target': np.int8, 'id': np.int32})\n",
    "    train = train_loader.drop(['target', 'id'], axis=1)\n",
    "    train_labels = train_loader['target'].values\n",
    "    train_ids = train_loader['id'].values\n",
    "    print('\\n Shape of raw train data:', train.shape)\n",
    "\n",
    "    test_loader = pd.read_csv(path_test, dtype={'id': np.int32})\n",
    "    test = test_loader.drop(['id'], axis=1)\n",
    "    test_ids = test_loader['id'].values\n",
    "    print(' Shape of raw test data:', test.shape)\n",
    "\n",
    "    return train, train_labels, test, train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Shape of raw train data: (595212, 57)\n",
      " Shape of raw test data: (892816, 57)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target_encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-63b8e9e97285>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf_cats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n\u001b[0m\u001b[0;32m     11\u001b[0m                                          \u001b[0mtst_series\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                          \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_encode' is not defined"
     ]
    }
   ],
   "source": [
    "# # Load data set and target values\n",
    "# train, target, test, tr_ids, te_ids = load_data()\n",
    "# target_df = pd.DataFrame()\n",
    "# target_df['target'] = target\n",
    "# n_train = train.shape[0]\n",
    "\n",
    "# f_cats = [f for f in train.columns if \"_cat\" in f]\n",
    "\n",
    "# for f in f_cats:\n",
    "#     train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "#                                          tst_series=test[f],\n",
    "#                                          target=target_df.target,\n",
    "#                                          min_samples_leaf=200,\n",
    "#                                          smoothing=10,\n",
    "#                                          noise_level=0)\n",
    "\n",
    "# train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "# calc_col = [c for c in train.columns if c.startswith('ps_calc_')]\n",
    "# train_test.drop(calc_col, axis=1, inplace=True)\n",
    "\n",
    "# train_test_scaled, scaler = scale_data(train_test)\n",
    "# train = train_test_scaled[:n_train, :]\n",
    "# test = train_test_scaled[n_train:, :]\n",
    "# print('\\n Shape of processed train data:', train.shape)\n",
    "# print(' Shape of processed test data:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Shape of raw train data: (595212, 57)\n",
      " Shape of raw test data: (892816, 57)\n",
      "\n",
      " Shape of processed train data: (595212, 207)\n",
      " Shape of processed test data: (892816, 207)\n"
     ]
    }
   ],
   "source": [
    "# Load data set and target values\n",
    "train, target, test, tr_ids, te_ids = load_data()\n",
    "n_train = train.shape[0]\n",
    "train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "calc_col = [c for c in train.columns if c.startswith('ps_calc_')]\n",
    "train_test.drop(calc_col, axis=1, inplace=True)\n",
    "\n",
    "col_to_drop = train.columns[train.columns.str.endswith('_cat')]\n",
    "col_to_dummify = train.columns[train.columns.str.endswith('_cat')].astype(str).tolist()\n",
    "\n",
    "for col in col_to_dummify:\n",
    "    dummy = pd.get_dummies(train_test[col].astype('category'))\n",
    "    columns = dummy.columns.astype(str).tolist()\n",
    "    columns = [col + '_' + w for w in columns]\n",
    "    dummy.columns = columns\n",
    "    train_test = pd.concat((train_test, dummy), axis=1)\n",
    "\n",
    "train_test.drop(col_to_dummify, axis=1, inplace=True)\n",
    "train_test_scaled, scaler = scale_data(train_test)\n",
    "train = train_test_scaled[:n_train, :]\n",
    "test = train_test_scaled[n_train:, :]\n",
    "print('\\n Shape of processed train data:', train.shape)\n",
    "print(' Shape of processed test data:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(y, pred):\n",
    "    fpr, tpr, thr = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    g = 2 * metrics.auc(fpr, tpr) -1\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu(0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ctx = mx.gpu()\n",
    "    _ = nd.zeros((1,), ctx=ctx)\n",
    "except:\n",
    "    ctx = mx.cpu()\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Dense(200, activation=\"relu\"))\n",
    "        net.add(gluon.nn.BatchNorm())\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "\n",
    "        net.add(gluon.nn.Dense(100, activation=\"relu\"))\n",
    "        net.add(gluon.nn.BatchNorm())\n",
    "        net.add(gluon.nn.Dropout(0.25))\n",
    "\n",
    "        net.add(gluon.nn.Dense(50, activation=\"relu\"))\n",
    "        net.add(gluon.nn.BatchNorm())\n",
    "        net.add(gluon.nn.Dropout(0.15))\n",
    "\n",
    "        net.add(gluon.nn.Dense(25, activation=\"relu\"))\n",
    "        net.add(gluon.nn.BatchNorm())\n",
    "        net.add(gluon.nn.Dropout(0.1))\n",
    "\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.astype('float32')\n",
    "test = test.astype('float32')\n",
    "target = target.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(dataset, net):\n",
    "    losses = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for data, label in dataset:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        loss = sigmoid_cross_entropy(output, label)\n",
    "        losses.append(nd.mean(loss).asscalar())\n",
    "        preds += nd.sigmoid(output).asnumpy().reshape((-1)).tolist()\n",
    "        labels += label.asnumpy().reshape((-1)).tolist()\n",
    "    score = gini(labels, preds)\n",
    "    loss = np.mean(losses)\n",
    "    return score, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:98: DeprecationWarning: DisplayFormatter._formatters_default is deprecated: use @default decorator instead.\n",
      "  def _formatters_default(self):\n",
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n",
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(595212, 207)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold: 0 Bag: 0\n",
      "epoch 0  train-gini: 0.2456  val-gini: 0.2451\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 1  train-gini: 0.2673  val-gini: 0.2549\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 2  train-gini: 0.2736  val-gini: 0.2619\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 3  train-gini: 0.2772  val-gini: 0.2612\n",
      "epoch 4  train-gini: 0.2824  val-gini: 0.2611\n",
      "epoch 5  train-gini: 0.2838  val-gini: 0.2622\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 6  train-gini: 0.2847  val-gini: 0.2650\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 7  train-gini: 0.2915  val-gini: 0.2639\n",
      "epoch 8  train-gini: 0.2945  val-gini: 0.2684\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 9  train-gini: 0.2972  val-gini: 0.2706\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 10  train-gini: 0.2973  val-gini: 0.2692\n",
      "epoch 11  train-gini: 0.3020  val-gini: 0.2698\n",
      "epoch 12  train-gini: 0.3057  val-gini: 0.2708\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 13  train-gini: 0.3080  val-gini: 0.2718\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 14  train-gini: 0.3096  val-gini: 0.2703\n",
      "epoch 15  train-gini: 0.3106  val-gini: 0.2707\n",
      "epoch 16  train-gini: 0.3108  val-gini: 0.2702\n",
      "epoch 17  train-gini: 0.3134  val-gini: 0.2715\n",
      "epoch 18  train-gini: 0.3148  val-gini: 0.2733\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 19  train-gini: 0.3171  val-gini: 0.2711\n",
      "epoch 20  train-gini: 0.3169  val-gini: 0.2736\n",
      "save to: mx_5fold_fold0_bag0.params\n",
      "epoch 21  train-gini: 0.3189  val-gini: 0.2711\n",
      "epoch 22  train-gini: 0.3229  val-gini: 0.2695\n",
      "epoch 23  train-gini: 0.3221  val-gini: 0.2693\n",
      "epoch 24  train-gini: 0.3245  val-gini: 0.2688\n",
      "epoch 25  train-gini: 0.3254  val-gini: 0.2709\n",
      "epoch 26  train-gini: 0.3225  val-gini: 0.2660\n",
      "epoch 27  train-gini: 0.3258  val-gini: 0.2713\n",
      "epoch 28  train-gini: 0.3236  val-gini: 0.2696\n",
      "early stop, best gini: 0.2736\n",
      "\n",
      "\n",
      "Fold: 1 Bag: 0\n",
      "epoch 0  train-gini: 0.2564  val-gini: 0.2492\n",
      "save to: mx_5fold_fold1_bag0.params\n",
      "epoch 1  train-gini: 0.2678  val-gini: 0.2580\n",
      "save to: mx_5fold_fold1_bag0.params\n",
      "epoch 2  train-gini: 0.2717  val-gini: 0.2528\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "skf = StratifiedKFold(n_splits=5, random_state=1001)\n",
    "increase = False\n",
    "best_ginis = []\n",
    "num_bagging = 1\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(train, target)):\n",
    "    for j in range(num_bagging):\n",
    "        train_X, train_y = train[train_index], target[train_index]\n",
    "        valid_X, valid_y = train[valid_index], target[valid_index]\n",
    "\n",
    "        if increase:\n",
    "            # Get positive examples\n",
    "            pos = (train_y == 1)\n",
    "            # Add positive examples\n",
    "            train_X = np.concatenate([train_X, train_X[pos]], axis=0)\n",
    "            train_y = np.concatenate([train_y, train_y[pos]], axis=0)\n",
    "            # Shuffle data\n",
    "            idx = np.arange(len(train_X))\n",
    "            np.random.shuffle(idx)\n",
    "            train_X = train_X[idx]\n",
    "            train_y = train_y[idx]\n",
    "\n",
    "        train_dataset = gluon.data.ArrayDataset(train_X, train_y)\n",
    "        train_data = gluon.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "        valid_dataset = gluon.data.ArrayDataset(valid_X, valid_y)\n",
    "        valid_data = gluon.data.DataLoader(valid_dataset, batch_size, shuffle=True)\n",
    "\n",
    "        net = get_net()\n",
    "        net.initialize(ctx=ctx)\n",
    "        sigmoid_cross_entropy = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "        trainer = gluon.Trainer(net.collect_params(), 'adam')\n",
    "\n",
    "        best_gini = -999\n",
    "        early_stop_round = 7\n",
    "        early_stop_counter = 0\n",
    "        print('\\n\\nFold:', i, 'Bag:', j)\n",
    "        for epoch in range(1000):\n",
    "            for data, label in train_data:\n",
    "                data = data.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                with autograd.record():\n",
    "                    output = net(data)\n",
    "                    loss = sigmoid_cross_entropy(output, label)\n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "\n",
    "            train_gini, train_loss = evaluation(train_data, net)\n",
    "            valid_gini, valid_loss = evaluation(valid_data, net)\n",
    "            print('epoch %d  train-gini: %.4f  val-gini: %.4f' % (epoch, train_gini, valid_gini))\n",
    "            if valid_gini > best_gini:\n",
    "                best_gini = valid_gini\n",
    "                early_stop_counter = 0\n",
    "                filename = 'mx_5fold_fold{}_bag{}.params'.format(i, j)\n",
    "                net.save_params(filename)\n",
    "                print('save to:', filename)\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter > early_stop_round:\n",
    "                    print('early stop, best gini: %.4f' % (best_gini))\n",
    "                    best_ginis.append(best_gini)\n",
    "                    break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28122612915657313"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(best_ginis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((892816, 51), (595212, 51))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = gluon.data.DataLoader(test, batch_size, shuffle=False)\n",
    "train_data = gluon.data.DataLoader(train, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(5):\n",
    "    for j in range(3):\n",
    "        filename = './mx_5fold_fold{}_bag{}.params'.format(i, j)\n",
    "        net = get_net()\n",
    "        net.load_params(filename=filename, ctx=ctx)\n",
    "        pred = []\n",
    "        for data in test_data:\n",
    "            data = data.as_in_context(ctx)\n",
    "            output = net(data)\n",
    "            pred += nd.sigmoid(output).asnumpy().reshape((-1)).tolist()\n",
    "        preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_preds = []\n",
    "for i in range(5):\n",
    "    for j in range(3):\n",
    "        filename = './mx_5fold_fold{}_bag{}.params'.format(i, j)\n",
    "        net = get_net()\n",
    "        net.load_params(filename=filename, ctx=ctx)\n",
    "        pred = []\n",
    "        for data in train_data:\n",
    "            data = data.as_in_context(ctx)\n",
    "            output = net(data)\n",
    "            pred += nd.sigmoid(output).asnumpy().reshape((-1)).tolist()\n",
    "        train_preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = np.array(preds)\n",
    "train_preds = np.array(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred_df = pd.DataFrame()\n",
    "train_pred_df['target'] = train_preds.mean(axis=0)\n",
    "train_pred_df.to_csv('../output/stacking_mxnet1_cv2812_train.csv', index=False, float_format='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = te_ids\n",
    "sub['target'] = preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub.to_csv('../output/stacking_mxnet1_cv2812_test.csv', index=False, float_format='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
