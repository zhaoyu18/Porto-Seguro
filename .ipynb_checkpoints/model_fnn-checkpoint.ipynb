{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(y, pred):\n",
    "    fpr, tpr, thr = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    g = 2 * metrics.auc(fpr, tpr) -1\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and test data path\n",
    "DATA_TRAIN_PATH = '../input/train.csv'\n",
    "DATA_TEST_PATH = '../input/test.csv'\n",
    "\n",
    "def load_data(path_train=DATA_TRAIN_PATH, path_test=DATA_TEST_PATH):\n",
    "    train_loader = pd.read_csv(path_train, dtype={'target': np.int8, 'id': np.int32})\n",
    "    train = train_loader.drop(['target', 'id'], axis=1)\n",
    "    train_labels = train_loader['target'].values\n",
    "    train_ids = train_loader['id'].values\n",
    "    print('\\n Shape of raw train data:', train.shape)\n",
    "\n",
    "    test_loader = pd.read_csv(path_test, dtype={'id': np.int32})\n",
    "    test = test_loader.drop(['id'], axis=1)\n",
    "    test_ids = test_loader['id'].values\n",
    "    print(' Shape of raw test data:', test.shape)\n",
    "\n",
    "    return train, train_labels, test, train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Shape of raw train data: (595212, 57)\n",
      " Shape of raw test data: (892816, 57)\n"
     ]
    }
   ],
   "source": [
    "# Load data set and target values\n",
    "train, target, test, tr_ids, te_ids = load_data()\n",
    "n_train = train.shape[0]\n",
    "train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "calc_col = [c for c in train.columns if c.startswith('ps_calc_')]\n",
    "train_test.drop(calc_col, axis=1, inplace=True)\n",
    "\n",
    "# ps_ind_14 = (ps_ind_10 + ps_ind_11 + ps_ind_12 + ps_ind_13)\n",
    "# train_test.drop([\"ps_ind_14\"], axis=1, inplace=True)\n",
    "\n",
    "col_to_drop = train.columns[train.columns.str.endswith('_cat')]\n",
    "col_to_dummify = train.columns[train.columns.str.endswith('_cat')].astype(str).tolist()\n",
    "\n",
    "train_test_dummy_all_df = pd.DataFrame()\n",
    "train_test_dummy_dfs = []\n",
    "\n",
    "# dummy0 = train_test[[\"ps_ind_06_bin\", \"ps_ind_07_bin\", \"ps_ind_08_bin\", \"ps_ind_09_bin\"]]\n",
    "# train_test_dummy_dfs.append(dummy0)\n",
    "# train_test_dummy_all_df = pd.concat([train_test_dummy_all_df, dummy0], axis=1)\n",
    "# train_test.drop([\"ps_ind_06_bin\", \"ps_ind_07_bin\", \"ps_ind_08_bin\", \"ps_ind_09_bin\"], axis=1, inplace=True)\n",
    "\n",
    "for col in col_to_dummify:\n",
    "    dummy = pd.get_dummies(train_test[col].astype('category'))\n",
    "    columns = dummy.columns.astype(str).tolist()\n",
    "    columns = [col + '_' + w for w in columns]\n",
    "    dummy.columns = columns\n",
    "    train_test_dummy_dfs.append(dummy)\n",
    "    train_test_dummy_all_df = pd.concat([train_test_dummy_all_df, dummy], axis=1)\n",
    "\n",
    "train_test.drop(col_to_dummify, axis=1, inplace=True)\n",
    "train_test_con_scaled, scaler = scale_data(train_test)\n",
    "# train_test_con_scaled = np.concatenate((train_test_con_scaled, train_test_dummy_all_df.values), axis=1)\n",
    "# train = train_test_scaled[:n_train, :]\n",
    "# test = train_test_scaled[n_train:, :]\n",
    "# print('\\n Shape of processed train data:', train.shape)\n",
    "# print(' Shape of processed test data:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, (1488028, 23))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_test_dummy_dfs), train_test_con_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 5)\n",
      "(595212, 3)\n",
      "(595212, 8)\n",
      "(595212, 13)\n",
      "(595212, 3)\n",
      "(595212, 3)\n",
      "(595212, 10)\n",
      "(595212, 3)\n",
      "(595212, 18)\n",
      "(595212, 3)\n",
      "(595212, 2)\n",
      "(595212, 6)\n",
      "(595212, 3)\n",
      "(595212, 104)\n"
     ]
    }
   ],
   "source": [
    "train_con_scaled_data = train_test_con_scaled[:n_train, :]\n",
    "train_cat_datas = []\n",
    "dummy_field_sizes = []\n",
    "for dummy_df in train_test_dummy_dfs:\n",
    "    train_cat_data = dummy_df.values[:n_train, :]\n",
    "    train_cat_datas.append(train_cat_data)\n",
    "    dummy_field_sizes.append(train_cat_data.shape[1])\n",
    "    print(train_cat_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 8, 13, 3, 3, 10, 3, 18, 3, 2, 6, 3, 104]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_field_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import InputSpec, Layer, Input, Dense, merge, Conv1D\n",
    "from keras.layers import Lambda, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhaoy\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(con_size, cat_sizes, v_size):\n",
    "    inputs = []\n",
    "    concates = []\n",
    "    \n",
    "    seq_con = Input(shape=(con_size,))\n",
    "    inputs.append(seq_con)\n",
    "    concates.append(seq_con)\n",
    "    \n",
    "    for size in cat_sizes:\n",
    "        seq_cat = Input(shape=(size,))\n",
    "        cat = Dense(v_size, activation=\"relu\", kernel_regularizer=regularizers.l2(0.005))(seq_cat)\n",
    "        cat = BatchNormalization()(cat)\n",
    "        inputs.append(seq_cat)\n",
    "        concates.append(cat)\n",
    "    \n",
    "    merge = concatenate(concates)\n",
    "    \n",
    "    x = Dense(200, activation=\"relu\")(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.35)(x)\n",
    "    \n",
    "    x = Dense(100, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.15)(x)\n",
    "    \n",
    "    x = Dense(25, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "#     x = Dropout(rate=0.1)(x)\n",
    "    \n",
    "    pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfolds = 5\n",
    "folds = KFold(train_con_scaled_data.shape[0], n_folds = nfolds, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476169, 23) (476169,)\n",
      "Train on 476169 samples, validate on 119043 samples\n",
      "Epoch 1/20\n",
      "score:  0.223878886247 0.205648461547\n",
      "28s - loss: 0.2927 - val_loss: 0.1632\n",
      "Epoch 2/20\n",
      "score:  0.255955480464 0.237182557044\n",
      "24s - loss: 0.1577 - val_loss: 0.1534\n",
      "Epoch 3/20\n",
      "score:  0.247967852671 0.222160516997\n",
      "24s - loss: 0.1542 - val_loss: 0.1532\n",
      "Epoch 4/20\n",
      "score:  0.262939652747 0.247681276547\n",
      "24s - loss: 0.1538 - val_loss: 0.1527\n",
      "Epoch 5/20\n",
      "score:  0.266057623003 0.25074397052\n",
      "24s - loss: 0.1537 - val_loss: 0.1529\n",
      "Epoch 6/20\n",
      "score:  0.261905206815 0.237393543626\n",
      "24s - loss: 0.1535 - val_loss: 0.1527\n",
      "Epoch 7/20\n",
      "score:  0.268986471075 0.254458667539\n",
      "24s - loss: 0.1534 - val_loss: 0.1523\n",
      "Epoch 8/20\n",
      "score:  0.275320311332 0.250489899457\n",
      "24s - loss: 0.1532 - val_loss: 0.1525\n",
      "Epoch 9/20\n",
      "score:  0.276025623892 0.250898431719\n",
      "24s - loss: 0.1531 - val_loss: 0.1522\n",
      "Epoch 10/20\n",
      "score:  0.278351485061 0.258473761042\n",
      "24s - loss: 0.1531 - val_loss: 0.1522\n",
      "Epoch 11/20\n",
      "score:  0.282772455771 0.259690368921\n",
      "24s - loss: 0.1529 - val_loss: 0.1522\n",
      "Epoch 12/20\n",
      "score:  0.278057898304 0.256486903333\n",
      "24s - loss: 0.1529 - val_loss: 0.1522\n",
      "Epoch 13/20\n",
      "score:  0.282160554598 0.261551695315\n",
      "24s - loss: 0.1528 - val_loss: 0.1520\n",
      "Epoch 14/20\n",
      "score:  0.285896764023 0.267310246505\n",
      "24s - loss: 0.1529 - val_loss: 0.1522\n",
      "Epoch 15/20\n",
      "score:  0.286258145611 0.266473920327\n",
      "24s - loss: 0.1527 - val_loss: 0.1521\n",
      "Epoch 16/20\n",
      "score:  0.288175669783 0.268299527644\n",
      "24s - loss: 0.1528 - val_loss: 0.1519\n",
      "Epoch 17/20\n",
      "score:  0.287312406775 0.266851171356\n",
      "24s - loss: 0.1526 - val_loss: 0.1518\n",
      "Epoch 18/20\n",
      "score:  0.28927123989 0.263239856872\n",
      "24s - loss: 0.1526 - val_loss: 0.1519\n",
      "Epoch 19/20\n",
      "score:  0.289773578987 0.268075332667\n",
      "24s - loss: 0.1527 - val_loss: 0.1520\n",
      "Epoch 20/20\n",
      "score:  0.298803675331 0.272611497683\n",
      "24s - loss: 0.1525 - val_loss: 0.1518\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "bst_evaluate = 0\n",
    "\n",
    "for curr_fold, (idx_train, idx_val) in enumerate(folds):\n",
    "    train_fold_datas = []\n",
    "    train_fold_con_data = train_con_scaled_data[idx_train]\n",
    "    train_fold_datas.append(train_fold_con_data)\n",
    "    \n",
    "    for cat_df in train_cat_datas:\n",
    "        train_fold_cat_data = cat_df[idx_train]\n",
    "        train_fold_datas.append(train_fold_cat_data)\n",
    "    \n",
    "    train_fold_target = target[idx_train]\n",
    "    \n",
    "    val_fold_datas = []\n",
    "    val_fold_con_data = train_con_scaled_data[idx_val]\n",
    "    val_fold_datas.append(val_fold_con_data)\n",
    "    \n",
    "    for cat_df in train_cat_datas:\n",
    "        val_fold_cat_data = cat_df[idx_val]\n",
    "        val_fold_datas.append(val_fold_cat_data)\n",
    "    \n",
    "    val_fold_target = target[idx_val]\n",
    "    \n",
    "    print(train_fold_con_data.shape, train_fold_target.shape)\n",
    "    \n",
    "    model = build_model(con_size=train_fold_con_data.shape[1],\n",
    "                        cat_sizes=dummy_field_sizes,\n",
    "                        v_size=2,\n",
    "                       )\n",
    "    \n",
    "    def batch_evalue(batch, logs):\n",
    "        global model, valid_X, valid_y, bst_evaluate, scores\n",
    "        if logs['batch'] % 900 == 0 and logs['batch'] != 0:\n",
    "            val_pred = model.predict(val_fold_datas,\n",
    "                                 batch_size=2048,\n",
    "                                 verbose=2\n",
    "                                )\n",
    "            val_score = gini(val_fold_target, val_pred)\n",
    "            \n",
    "            train_pred = model.predict(train_fold_datas,\n",
    "                                 batch_size=2048,\n",
    "                                 verbose=2\n",
    "                                )\n",
    "            train_score = gini(train_fold_target, train_pred)\n",
    "            print(\"score: \", train_score, val_score)\n",
    "            scores.append((train_score, val_score))\n",
    "            \n",
    "    batch_callback = LambdaCallback(on_batch_begin=batch_evalue)\n",
    "    \n",
    "    hist = model.fit(train_fold_datas,\n",
    "                     train_fold_target,\n",
    "                     validation_data=(val_fold_datas, val_fold_target),\n",
    "                     epochs = 20,\n",
    "                     batch_size = 512,\n",
    "                     shuffle = True,\n",
    "                     verbose = 2,\n",
    "                     callbacks=[batch_callback]\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2238788862468164, 0.20564846154680394),\n",
       " (0.25595548046416261, 0.23718255704399405),\n",
       " (0.24796785267116817, 0.22216051699694694),\n",
       " (0.26293965274671227, 0.24768127654734884),\n",
       " (0.26605762300349078, 0.25074397052000919),\n",
       " (0.26190520681463036, 0.23739354362619691),\n",
       " (0.26898647107463836, 0.25445866753930613),\n",
       " (0.27532031133214496, 0.25048989945711786),\n",
       " (0.27602562389191054, 0.25089843171924198),\n",
       " (0.27835148506060103, 0.258473761041768),\n",
       " (0.282772455771229, 0.25969036892147024),\n",
       " (0.27805789830415439, 0.25648690333281254),\n",
       " (0.28216055459751677, 0.26155169531548106),\n",
       " (0.28589676402320929, 0.26731024650525814),\n",
       " (0.28625814561065566, 0.26647392032697326),\n",
       " (0.28817566978341702, 0.26829952764447884),\n",
       " (0.28731240677452141, 0.26685117135562875),\n",
       " (0.28927123988993375, 0.26323985687202067),\n",
       " (0.28977357898721268, 0.26807533266657724),\n",
       " (0.29880367533071617, 0.27261149768262105)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
